{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入transformers\r\n",
    "import transformers\r\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\r\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\r\n",
    "\r\n",
    "\r\n",
    "# 导入torch\r\n",
    "import torch\r\n",
    "from torch import nn, optim\r\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "\r\n",
    "# 常用包\r\n",
    "import numpy as np\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "from collections import defaultdict\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from imblearn.over_sampling import SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_num = 66\r\n",
    "n_num = 208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\r\n",
    "np.random.seed(RANDOM_SEED)\r\n",
    "torch.manual_seed(RANDOM_SEED)\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased' # 英文bert预训练模型\r\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\r\n",
    "    def __init__(self,texts,labels,tokenizer,max_len):\r\n",
    "        self.texts=texts\r\n",
    "        self.labels=labels\r\n",
    "        self.tokenizer=tokenizer\r\n",
    "        self.max_len=max_len\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.texts)\r\n",
    "    \r\n",
    "    def __getitem__(self,item):\r\n",
    "        \"\"\"\r\n",
    "        item 为数据索引，迭代取第item条数据\r\n",
    "        \"\"\"\r\n",
    "        text=str(self.texts[item])\r\n",
    "        label=self.labels[item]\r\n",
    "        \r\n",
    "        encoding=self.tokenizer.encode_plus(\r\n",
    "            text,\r\n",
    "            add_special_tokens=True,\r\n",
    "            max_length=self.max_len,\r\n",
    "            return_token_type_ids=True,\r\n",
    "            pad_to_max_length=True,\r\n",
    "            return_attention_mask=True,\r\n",
    "            return_tensors='pt',\r\n",
    "        )\r\n",
    "        \r\n",
    "#         print(encoding['input_ids'])\r\n",
    "        return {\r\n",
    "            'texts':text,\r\n",
    "            'input_ids':encoding['input_ids'].flatten(),\r\n",
    "            'attention_mask':encoding['attention_mask'].flatten(),\r\n",
    "            'labels':torch.tensor(label,dtype=torch.long)\r\n",
    "        }\r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df,y,tokenizer,max_len,batch_size):\r\n",
    "    ds=TextDataset(\r\n",
    "        texts=df,\r\n",
    "        labels=y,\r\n",
    "        tokenizer=tokenizer,\r\n",
    "        max_len=max_len\r\n",
    "    )\r\n",
    "    \r\n",
    "    return DataLoader(\r\n",
    "        ds,\r\n",
    "        batch_size=batch_size,\r\n",
    "#         num_workers=4 # windows多线程\r\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\r\n",
    "    def __init__(self, n_classes):\r\n",
    "        super(TextClassifier, self).__init__()\r\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n",
    "        self.drop = nn.Dropout(p=0.3)\r\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n",
    "    def forward(self, input_ids, attention_mask):\r\n",
    "        _, pooled_output = self.bert(\r\n",
    "            input_ids=input_ids,\r\n",
    "            attention_mask=attention_mask,\r\n",
    "            return_dict = False\r\n",
    "        )\r\n",
    "        output = self.drop(pooled_output) # dropout\r\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(2) #二分类\r\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\r\n",
    "    model = model.eval()\r\n",
    "\r\n",
    "    texts = []\r\n",
    "    predictions = []\r\n",
    "    prediction_probs = []\r\n",
    "    real_values = []\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for d in data_loader:\r\n",
    "            texts = d[\"texts\"]\r\n",
    "            input_ids = d[\"input_ids\"].to(device)\r\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\r\n",
    "            targets = d[\"labels\"].to(device)\r\n",
    "\r\n",
    "            outputs = model(\r\n",
    "                input_ids=input_ids,\r\n",
    "                attention_mask=attention_mask\r\n",
    "            )\r\n",
    "            _, preds = torch.max(outputs, dim=1)\r\n",
    "\r\n",
    "            probs = F.softmax(outputs, dim=1)\r\n",
    "\r\n",
    "            texts.extend(texts)\r\n",
    "            predictions.extend(preds)\r\n",
    "            prediction_probs.extend(probs)\r\n",
    "            real_values.extend(targets)\r\n",
    "\r\n",
    "    predictions = torch.stack(predictions).cpu()\r\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\r\n",
    "    real_values = torch.stack(real_values).cpu()\r\n",
    "    return texts, predictions, prediction_probs, real_values\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\r\n",
    "model_dict = model.load_state_dict(torch.load('bert/best_model_state.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1, 0.8433834314346313)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def demo(test):\r\n",
    "    \"\"\"\r\n",
    "    parameter\r\n",
    "    ---------\r\n",
    "    test:字符串，英文\r\n",
    "    \r\n",
    "    return\r\n",
    "    ------\r\n",
    "    result:判断结果，0或1，分别表示无抑郁/抑郁\r\n",
    "    prob:置信度，result的概率值，0~1范围内的浮点数\r\n",
    "    \"\"\"\r\n",
    "    df = np.array([test])\r\n",
    "    y = np.array([0])\r\n",
    "    demo_loader = create_data_loader(df, y, tokenizer, MAX_LEN, BATCH_SIZE)\r\n",
    "    y_texts_, y_pred_, y_pred_probs_, y_test_ = get_predictions(\r\n",
    "    model,\r\n",
    "    demo_loader\r\n",
    "    )\r\n",
    "    result = int(y_pred_[0])\r\n",
    "    prob = float(y_pred_probs_[0][result])\r\n",
    "    return result,prob\r\n",
    "\r\n",
    "# test = \"Hello world\"\r\n",
    "test = \"I like death\"\r\n",
    "demo(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}