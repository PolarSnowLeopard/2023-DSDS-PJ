{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>心理状态分析V1</h1>\n",
    "<ul>\n",
    "<li><h3>基于LSTM</h3></li>\n",
    "<li><h3>微博评论数据集</h3></li>\n",
    "<li><h3>标记negative(0)与positive(1)</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.读取数据</h2>\n",
    "<h3>1.1导入工具包</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2读取数据</h3>\n",
    "<h4>读取训练数据</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv(r'.\\data\\weibo_senti_100k.csv') #导入微博评论数据\r\n",
    "train_csv = mydata[12000:-12000]\r\n",
    "train_csv = train_csv.reset_index()\r\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>读取测试数据</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = mydata[:12000].append(mydata[-12000:]) #划分测试集\r\n",
    "test_csv = test_csv.reset_index()\r\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.数据预处理</h2>\n",
    "<h3>2.1评论文字预处理(规范化)</h3>\n",
    "<h4>定义评论文字预处理函数</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    '''对每条review中的不合法字符进行替换，并进行分词转化成词表'''\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    review_text = re.sub('@[\\u4e00-\\u9fa5]*|#(.)*#',\" \", review_text) #先去除用户名和话题标签\n",
    "    review_text = re.sub('[^\\u4e00-\\u9fa5]','',review_text) #再去除标点和英文\n",
    "    review_text = jieba.lcut(review_text) #分词\n",
    "    return review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对训练集进行处理\r\n",
    "train_label = train_csv['label'] #读取训练集中的'label'列\r\n",
    "y_train=train_label.values #将'label'列由series结构转换为ndarray结构\r\n",
    "train_texts = [] #二维列表，每个元素都是一条文本分词后的词语列表\r\n",
    "for i in range(len(train_csv['review'])):\r\n",
    "    train_texts.append(' '.join(review_to_wordlist(train_csv['review'][i]))) #逐个添加每条文本分词后的词语列表\r\n",
    "#显示示例\r\n",
    "print('评论内容：')\r\n",
    "print(train_texts[3])\r\n",
    "print('真实标签：')\r\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对测试集进行处理\r\n",
    "test_label=test_csv['label'] #读取测试集中的'label'列\r\n",
    "y_test=test_label.values #将'label'列由series结构转换为ndarray结构\r\n",
    "test_texts = [] #二维列表，每个元素都是一条文本分词后的词语列表\r\n",
    "for i in range(len(test_csv['review'])):\r\n",
    "    test_texts.append(' '.join(review_to_wordlist(test_csv['review'][i]))) #逐个添加每条文本分词后的词语列表\r\n",
    "print('评论内容：')\r\n",
    "print(test_texts[3])\r\n",
    "print('真实标签：')\r\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2将评论中划分的词语映射为数字</h3>\n",
    "<h4>利用keras里的token实现，首先用Tokenizer的 fit_on_texts 方法学习出文本的字典，然后通过texts_to_sequences实现对应的单词和数字的映射，最后通过pad_sequences方法补成同样长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立词库：从所有评论中找出出现频率表最多的5000个词\n",
    "token=tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
    "token.fit_on_texts(train_texts)\n",
    "word_index = token.word_index #word_index字典将评论内容中的每一个词都赋予一个数字，数字越小表明该词出现频率越高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据上面生成的字典word_index将每条评论的每个单词映射为一个数字\n",
    "train_sequences=token.texts_to_sequences(train_texts) #得到训练集文本文字对应的数字列表\n",
    "test_sequences=token.texts_to_sequences(test_texts) #得到测试集文本文字对应的数字列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#显示转换前的文字\n",
    "print(train_texts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#显示转换后的数字列表\n",
    "print(train_sequences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#确定训练集与测试集中的最长评论长度\n",
    "reviews_lens = map(len,train_texts+test_texts) #确定最长评论词数\n",
    "reviews_len_max=max(reviews_lens)\n",
    "reviews_len_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>查看训练集中转化后的评论和标签</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将每条评论都变为与最长评论相同长度的数字列表，不足的部分补0\n",
    "#padding='post'需要补0时在序列结尾补；truncating='post'需要截断时从结尾截断；maxlen是序列最大长度\n",
    "#还有一种方式,padding='pre'需要补0时在序列开头补；truncating='pre'需要截断时从开头截断\n",
    "x_train=tf.keras.preprocessing.sequence.pad_sequences(train_sequences,padding='post',truncating='post',maxlen=reviews_len_max)\n",
    "x_test=tf.keras.preprocessing.sequence.pad_sequences(test_sequences,padding='post',truncating='post',maxlen=reviews_len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3随机抽取构建训练集</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(train_texts)) #生成索引\n",
    "np.random.shuffle(index) #打乱索引\n",
    "print(x_train[index]) \n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.构架LSTM模型</h2>\n",
    "<h3>3.1使用tf.keras.models.Sequential()构建模型</h3>\n",
    "<h4>用layer来搭建顺序模型</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential([\r\n",
    "    #嵌入层：将已经数字化的文本转化为向量\r\n",
    "    #向量化目的：实现将词语嵌入多维矩阵，使得语义相近的词语，在空间距离上更接近\r\n",
    "    #设置输出的词向量维度为64，输入维度5000，每条评论长度与最长评论相同\r\n",
    "    tf.keras.layers.Embedding(output_dim=64,input_dim=5000,input_length=reviews_len_max),\r\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), #双向封装器，用于对序列进行前向和后向计算\r\n",
    "    tf.keras.layers.Dense(64, activation='relu'), #隐藏层，64个神经元，激活函数为relu函数\r\n",
    "    #Dropout层以指定的丢弃概率随机丢弃上一层神经元\r\n",
    "    tf.keras.layers.Dropout(0.2), #防止过拟合：随机选择丢弃20%神经元\r\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid') #输出层\r\n",
    "])\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2使用compile()配置训练算法</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr指学习率，epsilon指定一个较小的数代替0，防止实现过程除以0\n",
    "#decay为学习率在每轮训练的衰减因子，取值范围[0,1]，0代表学习率在训练过程中保持不变\n",
    "#经衰减后的学习率的计算公式为lr_i = lr_start * 1.0 / (1.0 + decay*i) , i为迭代周期，lr_start是lr的初始值\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.0001,epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, #指定优化器——adam\n",
    "              loss='binary_crossentropy', #指定损失函数——对数损失函数(针对二分类问题)\n",
    "              metrics=['accuracy']) #设置模型检验的方法——准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.训练模型</h2>\n",
    "<h3>4.1使用fit()训练模型</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>（可跳过训练直接加载训练数据）</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数返回：训练过程的数据记录history\n",
    "history = model.fit(x_train[index],y_train[index], #输入训练集的文字和标签\n",
    "                    validation_split=0.3, #取训练集30%数据当验证集\n",
    "                     epochs=10,batch_size=56,verbose=1,shuffle=True)\n",
    "#verbose=0表示不在标准输出流输出日志信息，verbose=1表示输出进度条记录，verbose=2表示为每个epoch输出一行记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练耗时较长，训练完后保存一次数据\n",
    "model.save_weights('./imdb-classify-lstm/finalmodel_weibo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e51cb79e80>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#恢复训练的数据\n",
    "model.load_weights('./imdb-classify-lstm/finalmodel_weibo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.2训练过程可视化</h3>\n",
    "<h4>绘制损失值变化过程</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-79aae8abc24a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#trainRecord.history为字典对象，包含训练过程中的loss和测量指标等记录项\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#模型在训练集上的精度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#模型在验证集上的精度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#模型在训练集上的损失值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#模型在验证集上的损失值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#trainRecord.history为字典对象，包含训练过程中的loss和测量指标等记录项\n",
    "train_acc = history.history['accuracy'] #模型在训练集上的精度\n",
    "val_acc = history.history['val_accuracy'] #模型在验证集上的精度\n",
    "train_loss = history.history['loss'] #模型在训练集上的损失值\n",
    "val_loss = history.history['val_loss'] #模型在验证集上的损失值\n",
    "\n",
    "epochs=range(1,len(train_acc)+1) #设置横坐标epochs的计算方法\n",
    "plt.figure(figsize=(9,7)) #设置图形的宽度和高度\n",
    "#绘制一条折线，以训练轮数epochs作为x轴，训练集的损失值train_loss作为y轴，线条颜色为红色，线条标签为'Training Loss'\n",
    "plt.plot(epochs,train_loss,'r',label='Training Loss') \n",
    "#绘制一条折线，以训练轮数epochs作为x轴，验证集的损失值val_loss作为y轴，线条颜色为蓝色，线条标签为'Validation Loss'\n",
    "plt.plot(epochs,val_loss,'b',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss') #图形的标题\n",
    "plt.xlabel('Epochs') #横坐标表示经过几轮迭代\n",
    "plt.ylabel('Loss') #纵坐标表示损失值\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>绘制准确度变化过程</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()  #清除所有轴，保持窗口打开\n",
    "plt.figure(figsize=(9,7)) #设置图形的宽度和高度\n",
    "plt.plot(epochs,train_acc,'r',label='Training Accuracy')\n",
    "plt.plot(epochs,val_acc,'b',label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs') #横坐标表示经过几轮迭代\n",
    "plt.ylabel('Accuracy') #纵坐标表示准确度\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.评估与预测</h2>\n",
    "<h3>5.1使用evaluate()预测并评估测试集的预测结果</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 18s 35ms/step - loss: 0.0968 - accuracy: 0.9687\n",
      "Test Loss: 0.09676802158355713\n",
      "Test Accuracy: 0.9687467217445374\n"
     ]
    }
   ],
   "source": [
    "#输入数据和标签，输出损失值和准确度\n",
    "test_loss,test_acc=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2使用predict()对测试集进行预测</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.9772314 ],\n       [0.98703027],\n       [0.99108505],\n       ...,\n       [0.00500336],\n       [0.00137082],\n       [0.00186062]], dtype=float32)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=model.predict(x_test) #输入测试集，返回预测结果\n",
    "predictions #显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将预测结果写入csv文件，包含文本内容，真实标签，预测标签\r\n",
    "with open(r'.\\data\\weibo_prediction.csv', 'w',encoding='utf-8-sig',newline='') as csvfile:\r\n",
    "    fieldnames = ['评论内容', '真实标签','预测标签']\r\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\r\n",
    "\r\n",
    "    writer.writeheader()\r\n",
    "    for i in range(len(test_texts)):\r\n",
    "        writer.writerow({'评论内容': test_texts[i], '真实标签': y_test[i],'预测标签': predictions[i][0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.3保存并显示测试集的预测结果</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>评论内容</th>\n      <th>真实标签</th>\n      <th>预测标签</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>更博 了 爆照 了 帅 的 呀 就是 越来越 爱 你 生快 傻 缺爱 你 爱 你 爱 你</td>\n      <td>1</td>\n      <td>0.977231</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>土耳其 的 事要 认真对待 哈哈 否则 直接 开除 很 是 细心 酒店 都 全部 啦</td>\n      <td>1</td>\n      <td>0.987030</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>姑娘 都 羡慕 你 呢 还有 招财猫 高兴 哈哈 小 学徒 一枚 等 着 明天 见 您 呢 ...</td>\n      <td>1</td>\n      <td>0.991085</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>美爱 你</td>\n      <td>1</td>\n      <td>0.502319</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>梦想 有 多 大 舞台 就 有 多 大 鼓掌</td>\n      <td>1</td>\n      <td>0.989366</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23995</th>\n      <td>一 公里 不到 县 医院 那个 天桥 下右 拐 米 就 到 了 我 靠 这个 太 霸道 了 ...</td>\n      <td>0</td>\n      <td>0.008835</td>\n    </tr>\n    <tr>\n      <th>23996</th>\n      <td>今天 真冷 啊 难道 又 要 穿 棉袄 了 晕 今年 的 春天 真的 是 百变 莫测 啊 抓狂</td>\n      <td>0</td>\n      <td>0.004863</td>\n    </tr>\n    <tr>\n      <th>23997</th>\n      <td>最近 几天 就 没 停止 过 伤心</td>\n      <td>0</td>\n      <td>0.005003</td>\n    </tr>\n    <tr>\n      <th>23998</th>\n      <td>怒 很惨</td>\n      <td>0</td>\n      <td>0.001371</td>\n    </tr>\n    <tr>\n      <th>23999</th>\n      <td>呢 抓狂 搞 乜 鬼 想 知入 去 睇 睇</td>\n      <td>0</td>\n      <td>0.001861</td>\n    </tr>\n  </tbody>\n</table>\n<p>24000 rows × 3 columns</p>\n</div>",
      "text/plain": "                                                    评论内容  真实标签      预测标签\n0           更博 了 爆照 了 帅 的 呀 就是 越来越 爱 你 生快 傻 缺爱 你 爱 你 爱 你     1  0.977231\n1             土耳其 的 事要 认真对待 哈哈 否则 直接 开除 很 是 细心 酒店 都 全部 啦     1  0.987030\n2      姑娘 都 羡慕 你 呢 还有 招财猫 高兴 哈哈 小 学徒 一枚 等 着 明天 见 您 呢 ...     1  0.991085\n3                                                   美爱 你     1  0.502319\n4                                 梦想 有 多 大 舞台 就 有 多 大 鼓掌     1  0.989366\n...                                                  ...   ...       ...\n23995  一 公里 不到 县 医院 那个 天桥 下右 拐 米 就 到 了 我 靠 这个 太 霸道 了 ...     0  0.008835\n23996    今天 真冷 啊 难道 又 要 穿 棉袄 了 晕 今年 的 春天 真的 是 百变 莫测 啊 抓狂     0  0.004863\n23997                                  最近 几天 就 没 停止 过 伤心     0  0.005003\n23998                                               怒 很惨     0  0.001371\n23999                              呢 抓狂 搞 乜 鬼 想 知入 去 睇 睇     0  0.001861\n\n[24000 rows x 3 columns]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取保存的csv文件并展示\n",
    "df = pd.read_csv(r'.\\data\\weibo_prediction.csv',encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.模型应用</h2>\n",
    "<h3>6.1定义预测结果显示函数</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_sentiment(text):\n",
    "    #显示要预测的文本内容\n",
    "    print(text)\n",
    "    #数据规范化\n",
    "    newtext=review_to_wordlist(text)\n",
    "    #将文本转换为数字序列\n",
    "    input_seq=token.texts_to_sequences([newtext])\n",
    "    #将序列填充或截断为固定长度的序列\n",
    "    pad_input_seq=tf.keras.preprocessing.sequence.pad_sequences(input_seq,\n",
    "                                                   padding='post',\n",
    "                                                   truncating='post',\n",
    "                                                   maxlen=reviews_len_max)\n",
    "    #使用训练好的模型进行预测\n",
    "    pred=model.predict(pad_input_seq)\n",
    "    print(pred[0][0])\n",
    "    #打印预测值\n",
    "    if pred[0][0]>0.5:\n",
    "        preValue='positive'\n",
    "    else:\n",
    "        preValue='negtive'\n",
    "    print('predict value:',preValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.2对输入评论进行预测</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "但是，最近我们发现有同学存在舞弊行为，有同学抄袭了别人的报告，有同学带着别人的报告在实验室做实验。对于这些行为，基础物理实验教学团队经讨论决定给予相关同学该实验0分的处理。希望我们所有同学以此为戒，从自身做起，恪守学术道德。\n",
      "\n",
      "0.14801186\n",
      "predict value: negtive\n"
     ]
    }
   ],
   "source": [
    "review_text='''但是，最近我们发现有同学存在舞弊行为，有同学抄袭了别人的报告，有同学带着别人的报告在实验室做实验。对于这些行为，基础物理实验教学团队经讨论决定给予相关同学该实验0分的处理。希望我们所有同学以此为戒，从自身做起，恪守学术道德。\n",
    "'''\n",
    "display_test_sentiment(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}