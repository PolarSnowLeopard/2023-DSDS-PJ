{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\r\n",
    "import re\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from torch.utils.data.dataset import random_split\r\n",
    "from torchtext.data.functional import to_map_style_dataset\r\n",
    "from torchtext import data\r\n",
    "from torchtext.vocab import build_vocab_from_iterator\r\n",
    "import jieba\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def tokenizer(text):\r\n",
    "    text = re.sub('@[\\u4e00-\\u9fa5]*|#(.)*#',\" \", text) #先去除用户名和话题标签\r\n",
    "    text = re.sub('[^\\u4e00-\\u9fa5]','',text) #再去除标点和英文\r\n",
    "    text = jieba.lcut(text) #分词\r\n",
    "    return text\r\n",
    "\r\n",
    "def yield_tokens(data_iter):\r\n",
    "    for _, text in data_iter.iterrows():\r\n",
    "        yield tokenizer(text['review'])\r\n",
    "\r\n",
    "raw_data = pd.read_csv(r'BaseCode\\WeiboSentimentClassification\\data\\weibo_senti_100k.csv')\r\n",
    "\r\n",
    "vocab = build_vocab_from_iterator(yield_tokens(raw_data), min_freq=5, specials=['<unk>', '<pad>'])\r\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = raw_data['label']\r\n",
    "text = [vocab(tokenizer(s)) for s in raw_data['review']]\r\n",
    "padded_text = torch.nn.utils.rnn.pad_sequence([torch.tensor(s,dtype=torch.float32) for s in text], batch_first=True).tolist()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNNDataSet(Dataset):\r\n",
    "    def __init__(self, data, data_targets):\r\n",
    "        self.content = data\r\n",
    "        self.pos = data_targets\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.content[index], self.pos[index]\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.pos)\r\n",
    "\r\n",
    "data_iter = TextCNNDataSet(padded_text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\r\n",
    "label_pipeline = lambda x: int(x) \r\n",
    "\r\n",
    "def collate_batch(batch):\r\n",
    "    label_list, text_list, lengths_list = [], [], []\r\n",
    "    for (_text, _label) in batch:\r\n",
    "        label_list.append(label_pipeline(_label))\r\n",
    "        text_list.append(_text)\r\n",
    "        \r\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32) \r\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)   \r\n",
    "\r\n",
    "\r\n",
    "    lengths_list = [len(sentence) for sentence in text_list]\r\n",
    "    lengths_list = torch.tensor(lengths_list, dtype=torch.int64)\r\n",
    "    return  text_list.to(device), label_list.to(device), lengths_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\r\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, sparse=False)\r\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\r\n",
    "        self.fc = nn.Linear(hidden_dim * 4, output_dim)\r\n",
    "        self.dropout = nn.Dropout(dropout)\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "\r\n",
    "        self.layers = nn.ModuleList([self.embedding, self.rnn, self.fc, self.dropout, self.sigmoid])\r\n",
    "\r\n",
    "    def forward(self, text, text_length):\r\n",
    "        embedded = self.embedding(text.T)\r\n",
    "        output, (hidden, cell) = self.rnn(embedded)\r\n",
    "        hidden = torch.cat((output[0], output[-1]), dim=1)\r\n",
    "        hidden = self.dropout(hidden)\r\n",
    "        result = self.fc(hidden.squeeze(0))\r\n",
    "        result = self.sigmoid(result)\r\n",
    "        return result\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\r\n",
    "EMBEDDING_DIM = 64\r\n",
    "BATCH_SIZE = 64\r\n",
    "HIDDEN_DIM = 64\r\n",
    "OUTPUT_DIM = 1\r\n",
    "N_LAYERS = 2\r\n",
    "BIDIRECTIONAL = True\r\n",
    "DROPOUT = 0.2\r\n",
    "\r\n",
    "LR=0.001\r\n",
    "\r\n",
    "MAX_LEN = len(padded_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysis(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\r\n",
    "criterion = nn.BCEWithLogitsLoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\r\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # 学习率衰减\r\n",
    "\r\n",
    "model = model.to(device)\r\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_acc = 0\r\n",
    "\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    for index, (text, label, text_lengths) in enumerate(dataloader):\r\n",
    "        optimizer.zero_grad()\r\n",
    "\r\n",
    "        predictions = model(text, text_lengths).squeeze(1)\r\n",
    "        loss = criterion(predictions, label)\r\n",
    "\r\n",
    "        # 计算模型在该batch上的acc\r\n",
    "        rounded_preds = torch.round(predictions)\r\n",
    "        # loss = criterion(rounded_preds, label)\r\n",
    "        correct = (rounded_preds == label).float()\r\n",
    "        acc = correct.sum() / len(correct)\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        # #梯度裁剪，防止梯度爆炸\r\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\r\n",
    "\r\n",
    "        epoch_loss += loss.item()\r\n",
    "        epoch_acc += acc.item()\r\n",
    "\r\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\r\n",
    "    model.eval()\r\n",
    "    total_loss = 0\r\n",
    "    total_acc = 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for index, (text, label, text_lengths) in enumerate(dataloader):\r\n",
    "            predictions = model(text, text_lengths).squeeze(1)\r\n",
    "            loss = criterion(predictions, label)\r\n",
    "\r\n",
    "            rounded_preds = torch.round(predictions)\r\n",
    "            correct = (rounded_preds == label).float()\r\n",
    "            acc = correct.sum() / len(correct)\r\n",
    "\r\n",
    "            total_loss += loss.item()\r\n",
    "            total_acc += acc.item()\r\n",
    "\r\n",
    "    return total_loss/len(dataloader), total_acc/len(dataloader)\r\n",
    "\r\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 \n",
      " Train Loss: 0.647 | Train Acc: 66.33%\n",
      "valid Loss: 0.603 | valid Acc: 77.88%\n",
      "Epoch: 02 \n",
      " Train Loss: 0.602 | Train Acc: 77.57%\n",
      "valid Loss: 0.587 | valid Acc: 81.25%\n",
      "Epoch: 03 \n",
      " Train Loss: 0.583 | Train Acc: 82.62%\n",
      "valid Loss: 0.578 | valid Acc: 82.69%\n",
      "Epoch: 04 \n",
      " Train Loss: 0.580 | Train Acc: 82.56%\n",
      "valid Loss: 0.572 | valid Acc: 86.59%\n",
      "Epoch: 05 \n",
      " Train Loss: 0.572 | Train Acc: 85.28%\n",
      "valid Loss: 0.572 | valid Acc: 84.91%\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data_iter)\r\n",
    "train_size = int(data_size * 0.80)\r\n",
    "valid_size = int(data_size * 0.15)\r\n",
    "test_size = data_size - train_size - valid_size\r\n",
    "\r\n",
    "data_set = to_map_style_dataset(data_iter)\r\n",
    "train_set, valid_set, test_set = random_split(data_set, [train_size, valid_size, test_size])\r\n",
    "\r\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,\r\n",
    "                              shuffle=True, collate_fn=collate_batch)\r\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE,\r\n",
    "                              shuffle=True, collate_fn=collate_batch)\r\n",
    "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,\r\n",
    "                             shuffle=True, collate_fn=collate_batch)\r\n",
    "\r\n",
    "N_EPOCHS = 5\r\n",
    "\r\n",
    "for epoch in range(N_EPOCHS):\r\n",
    "    \r\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\r\n",
    "    valid_loss, valid_acc = evaluate(valid_dataloader)\r\n",
    "\r\n",
    "    print(f'Epoch: {epoch+1:02} \\n Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
    "    print(f'valid Loss: {valid_loss:.3f} | valid Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 5114,   102,    14,  ...,     0,     0,     0],\n",
      "        [  171,   221,   339,  ...,     0,     0,     0],\n",
      "        [  284,  2628, 24332,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   37,     7,   965,  ...,     0,     0,     0],\n",
      "        [   26,   424,     3,  ...,     0,     0,     0],\n",
      "        [ 9062,   136,   474,  ...,     0,     0,     0]]), tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]), tensor([130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130,\n",
      "        130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130,\n",
      "        130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130,\n",
      "        130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130,\n",
      "        130, 130, 130, 130, 130, 130, 130, 130]))\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\r\n",
    "    print(batch)\r\n",
    "\r\n",
    "    print(batch[1].size())\r\n",
    "    break\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[1,2],\r\n",
    "[3,4]]\r\n",
    "\r\n",
    "a = torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[1, 2], [3, 4]]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\r\n",
    "# # tensorboard --logdir=./ --port 8123\r\n",
    "\r\n",
    "# # 创建SummaryWriter对象，log_dir为保存TensorBoard日志的路径\r\n",
    "# writer = SummaryWriter(log_dir='logs')\r\n",
    "\r\n",
    "# # 定义模型\r\n",
    "# model = SentimentAnalysis(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\r\n",
    "\r\n",
    "# # 将模型添加到TensorBoard中\r\n",
    "# text_ = torch.zeros(BATCH_SIZE, MAX_LEN).long()\r\n",
    "# text_length_ = torch.zeros(BATCH_SIZE).long()\r\n",
    "\r\n",
    "# writer.add_graph(model, (text_, text_length_))\r\n",
    "\r\n",
    "# # 关闭SummaryWriter对象\r\n",
    "# writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5fdd93fc8f1865df9116eb401350222b967fb9a256b1f03618630627bdf00f84"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}